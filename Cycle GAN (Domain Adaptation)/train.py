import torch #
import config #
from dataset import hzDataset #
from utils import save_checkpoint, load_checkpoint #
from torch.utils.data import DataLoader #
from tqdm import tqdm #
from torchvision.datasets import ImageFolder
import torch.nn as nn  #
from torchvision.utils import save_image #
from Discriminator_model import Discriminator #
from Generator_model import Generator #
import torch.optim as optim #
from torch.cuda.amp.grad_scaler import GradScaler


def train(disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler):

    loop = tqdm(loader, leave=True)   #leave-?

    for index, (horse,zebra) in enumerate(loop):

        zebra=zebra.to(config.DEVICE)
        horse=horse.to(config.DEVICE)

        #\training H and Z discriminator

        with torch.cuda.amp.autocast():

            #disc loss- between disc sigmoid output and torch.ones/zeros_like
            
            Generated_horse = gen_H(zebra) #generated image of horse given a zebra image

            D_H_real = disc_H(horse)   #real image of horse passed through the disc_H
            D_H_fake = disc_H(Generated_horse.detach()) #generated image of horse given a zebra image passed through the disc_H

            D_H_realLoss = mse(D_H_real, torch.ones_like(D_H_real)) #real loss
            D_H_fakeLoss = mse(D_H_fake, torch.zeros_like(D_H_fake)) #fake loss

            Disc_H_loss = D_H_realLoss + D_H_fakeLoss  #torch disc_H loss

            Generated_zebra = gen_Z(horse) #generated image of zebra image given a horse image

            D_Z_real = disc_Z(zebra)
            D_Z_fake = disc_Z(Generated_zebra.detach())

            D_Z_realLoss = mse(D_Z_real, torch.ones_like(D_Z_real))
            D_Z_fakeLoss = mse(D_Z_fake, torch.zeros_like(D_Z_fake))

            Disc_Z_loss = D_Z_realLoss + D_Z_fakeLoss #total Disc_z loss

            Disc_loss = (Disc_H_loss + Disc_Z_loss)/2  #/2 mentioned in the paper

        opt_disc.zero_grad()
        d_scaler.scale(Disc_loss).backward()
        d_scaler.step(opt_disc)
        d_scaler.update()

        with torch.cuda.amp.autocast():

            #adversarial loss- between disc sigmoid output and torch.ones/zeros_like

            disc_H_generated = disc_H(Generated_horse)   #disc output on generated horse image
            disc_Z_generated = disc_Z(Generated_zebra)   #disc output on generated zebra image

            #gen trying to classify generated images as real ones

            gen_H_adv = mse(disc_H_generated, torch.ones_like(disc_H_generated))  # loss bw disc output and 0's and 1's
            gen_Z_adv = mse(disc_Z_generated, torch.ones_like(disc_Z_generated))

            #cycle loss- between images

            h_dash = gen_H(Generated_zebra)  #generating horse image from generated 
            z_dash = gen_Z(Generated_horse)  #generating zebra from generated horse image

            cyc_H_loss = L1(horse, h_dash)  #mae between real and generated horse/zebra images
            cyc_Z_loss = L1(zebra, z_dash)  

            #identity loss- it is calculated b/w the real h/z images and the images generated by the gen_z/gen_h given z/h images respectively.

            ##Note: for h-z cycleGAN LAMDA_IDENTITY is set to be zero so no identity loss will be added to Gen_loss

            gen_HH = gen_H(horse)  #generating image from horse generator given horse image as input
            gen_ZZ = gen_Z(zebra)  #generating image from zebra generator given zebra image as input

            id_H_loss = L1(horse, gen_HH)
            id_Z_loss = L1(zebra, gen_ZZ)


            ##all together makes generator loss

            Gen_loss =  ( gen_H_adv + gen_Z_adv 
            + cyc_H_loss * config.LAMBDA_CYCLE + cyc_Z_loss * config.LAMBDA_CYCLE
            + id_H_loss * config.LAMBDA_IDENTITY + id_Z_loss * config.LAMBDA_IDENTITY
            )

        opt_gen.zero_grad()
        g_scaler.scale(Gen_loss).backward()
        g_scaler.step(opt_gen)
        g_scaler.update()

        if index % 200 == 0:   #save images after every 200 indexes
            save_image(Generated_horse*0.5+0.5, f"saved_images/horse_{index}.png")
            save_image(Generated_zebra*0.5+0.5, f"saved_images/zebra_{index}.png")  #*0.5+0.5 to reverse the normalization in the configs





def main():

    print(config.DEVICE)

    #initializing disc and gen

    disc_H = Discriminator(input_channels=3).to(config.DEVICE)
    disc_Z = Discriminator(input_channels=3).to(config.DEVICE)
    gen_H = Generator(input_channels=3,num_of_residuals=9).to(config.DEVICE)
    gen_Z = Generator(input_channels=3,num_of_residuals=9).to(config.DEVICE)

    #initializing optimizers

    opt_disc=torch.optim.Adam(

        list(disc_H.parameters())+list(disc_Z.parameters()),
        lr= config.LEARNING_RATE,
        betas=(0.5, 0.999),

    )

    opt_gen=torch.optim.Adam(

        list(gen_H.parameters())+list(gen_Z.parameters()),
        lr= config.LEARNING_RATE,
        betas=(0.5, 0.999),

    )

    #initializing losses

    L1 = nn.L1Loss()   #for cycle consistency and Identity loss 
    mse = nn.MSELoss() #Adversarial loss

    #model_loading

    if config.LOAD_MODEL:

        load_checkpoint(
            config.CHECKPOINT_GEN_H,gen_H, opt_gen, lr = config.LEARNING_RATE,
        )

        load_checkpoint(
            config.CHECKPOINT_GEN_Z,gen_Z, opt_gen, lr = config.LEARNING_RATE,
        )

        load_checkpoint(
            config.CHECKPOINT_DISC_H,disc_H, opt_disc, lr = config.LEARNING_RATE,
        )

        load_checkpoint(
            config.CHECKPOINT_DISC_Z,disc_Z, opt_disc, lr = config.LEARNING_RATE,
        )

    
    #loading dataset

    dataset = hzDataset(
        root_dir_horse= config.TRAIN_DIR+"/horse", root_dir_zebra= config.TRAIN_DIR+"/zebra",
        transform = config.transforms,
    )

    #dataloader

    loader = DataLoader(
        dataset, 
        batch_size= config.BATCH_SIZE, 
        shuffle=True, 
        num_workers=config.NUM_WORKERS, 
        pin_memory=True,
    )

    g_scaler = GradScaler()
    d_scaler = GradScaler()

    #training loop

    for epoch in range(config.NUM_EPOCHS):

        train(disc_H, disc_Z, gen_Z, gen_H, loader, opt_disc, opt_gen, L1, mse, d_scaler, g_scaler) 


        #saving checkpoint after every epoch

        if config.SAVE_MODEL:
            save_checkpoint(gen_H, opt_gen, filename= config.CHECKPOINT_GEN_H)
            save_checkpoint(gen_Z, opt_gen, filename= config.CHECKPOINT_GEN_Z)
            save_checkpoint(disc_H, opt_disc, filename= config.CHECKPOINT_DISC_H)
            save_checkpoint(disc_Z, opt_disc, filename= config.CHECKPOINT_DISC_Z)
            

if __name__ == "__main__":
    main()